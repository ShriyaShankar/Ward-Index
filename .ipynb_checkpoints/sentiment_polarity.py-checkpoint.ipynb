{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/shriya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "import os\n",
    "import warnings\n",
    "import nltk\n",
    "import string\n",
    "import collections\n",
    "import matplotlib.cm as cm\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from textblob import TextBlob,Word,Blobber\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "nltk.download('stopwords')\n",
    "warnings.filterwarnings('ignore')\n",
    "filepath = os.path.join(os.getcwd(),'CSV_Files','ninja_reports.xls')\n",
    "df = pd.read_excel('ninja_reports2.xls',sheet_name='ninja_reports')\n",
    "\n",
    "df_w = pd.read_csv(\"ward_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files_pos = os.listdir('pos.txt')\n",
    "#files_pos = open(\"pos.txt\", \"r\")\n",
    "#files_neg = open(\"neg.txt\", \"r\")\n",
    "training_set = open('training_set.txt', 'r')\n",
    "\n",
    "all_words = []\n",
    "documents = []\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "#  j is adject, r is adverb, and v is verb\n",
    "allowed_word_types = [\"J\",\"R\",\"V\"]\n",
    "#allowed_word_types = [\"J\"]\n",
    "count_p = count_n = 0\n",
    "for p in  training_set:\n",
    "    \n",
    "    # create a list of tuples where the first element of each tuple is a review\n",
    "    # the second element is the label\n",
    "    positive = float(p.split(',')[1])\n",
    "    if positive > 0.4:\n",
    "        documents.append( (p, \"pos\") )\n",
    "        count_p += 1\n",
    "    else:\n",
    "        documents.append( (p, \"neg\") )\n",
    "        count_n += 1\n",
    "    \n",
    "    # remove punctuations\n",
    "    cleaned = re.sub(r'[^(a-zA-Z)\\s]','', p)\n",
    "    \n",
    "    # tokenize \n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    \n",
    "    # remove stopwords \n",
    "    stopped = [w for w in tokenized if not w in stop_words]\n",
    "    \n",
    "    # parts of speech tagging for each word \n",
    "    pos = nltk.pos_tag(stopped)\n",
    "    \n",
    "    # make a list of  all adjectives identified by the allowed word types list above\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())\n",
    "\n",
    "    \n",
    "#print(all_words)\n",
    "#print(documents)\n",
    "#print(count_p, count_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pickling the list documents to save future recalculations \n",
    "filename = \"pickled_algos/documents.pickle\"\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "save_documents = 'documents'\n",
    "with open(\"pickled_algos/documents.pickle\",\"wb\") as f:\n",
    "    pickle.dump(save_documents, f)\n",
    "\n",
    "# creating a frequency distribution of each adjectives. \n",
    "BOW = nltk.FreqDist(all_words)\n",
    "BOW\n",
    "\n",
    "# listing the 5000 most frequent words\n",
    "word_features = list(BOW.keys())[:500]\n",
    "word_features[0], word_features[-1]\n",
    "\n",
    "save_word_features = open(\"pickled_algos/word_features5k.pickle\",\"wb\")\n",
    "pickle.dump(word_features, save_word_features)\n",
    "save_word_features.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a dictionary of features for each review in the list document.\n",
    "# The keys are the words in word_features \n",
    "# The values of each key are either true or false for wether that feature appears in the review or not\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Creating features for each review\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "# Shuffling the documents \n",
    "random.shuffle(featuresets)\n",
    "#print(len(featuresets))\n",
    "\n",
    "training_set = featuresets\n",
    "#print(len(training_set))\n",
    "#testing_set = featuresets[40:]\n",
    "#print( 'training_set :', len(training_set), '\\ntesting_set :', len(testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set : 40 \n",
      "testing_set : 12\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 90.47619047619048\n",
      "Most Informative Features\n",
      "                dustbins = True              pos : neg    =      6.0 : 1.0\n",
      "                    next = True              pos : neg    =      3.6 : 1.0\n",
      "                   right = True              pos : neg    =      2.6 : 1.0\n",
      "                  thrown = True              pos : neg    =      2.0 : 1.0\n",
      "                  filled = False             neg : pos    =      2.0 : 1.0\n",
      "                 plastic = False             neg : pos    =      1.4 : 1.0\n",
      "                  dumped = False             pos : neg    =      1.3 : 1.0\n",
      "                  coming = False             neg : pos    =      1.2 : 1.0\n",
      "                    main = False             neg : pos    =      1.2 : 1.0\n",
      "                    cows = False             neg : pos    =      1.2 : 1.0\n",
      "                  eating = False             neg : pos    =      1.2 : 1.0\n",
      "                  parked = False             neg : pos    =      1.2 : 1.0\n",
      "                dustbins = False             neg : pos    =      1.2 : 1.0\n",
      "                    next = False             neg : pos    =      1.1 : 1.0\n",
      "                   right = False             neg : pos    =      1.1 : 1.0\n",
      "['dustbins', 'next', 'right', 'thrown', 'filled', 'plastic', 'dumped', 'coming', 'main', 'cows', 'eating', 'parked', 'dustbins', 'next', 'right', 'thrown', 'dump', 'not', 'left', 'petty', 'happening', 'surrounding', 'putting', 'dry', 'badly', 'really', 'looks', 'bsnl', 'find', 'started', 'empty', 'stretched', 'horrible', 'keep', 'bad', 'long', 'vacant', 'waking', 'litter', 'smells', 'storm', 'picking', 'opposite', 'big', 'blocking', 'disposed', 'regular', 'infront', 'pile', 'leaves', 'huge', 'extremely', 'forward', 'smelly', 'unchecked', 'clear', 'breeding', 'playing', 'small', 'located', 'often', 'harmful', 'illegal', 'cleaned', 'cleared', 'segregated', 'dumping', 'due', 'huge', 'atthis', 'vacant', 'litter', 'disposable', 'extremely', 'dumping', 'clear', 'small', 'continuous', 'dumped', 'located', 'parked', 'unsegregated', 'left', 'cleared', 'filled', 'leaves', 'dry', 'cows', 'really', 'seen', 'looks', 'bsnl', 'not', 'main', 'forward', 'happy', 'smelly', 'plastic', 'empty', 'unchecked']\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "# Printing the most important features \n",
    "\n",
    "mif = classifier.most_informative_features()\n",
    "\n",
    "mif = [a for a,b in mif]\n",
    "print(mif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9047619047619048"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting predictions for the testing set by looping over each reviews featureset tuple\n",
    "# The first elemnt of the tuple is the feature set and the second element is the label \n",
    "ground_truth = [r[1] for r in testing_set]\n",
    "\n",
    "preds = [classifier.classify(r[0]) for r in testing_set]\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(ground_truth, preds, labels = ['neg', 'pos'], average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 90.47619047619048\n",
      "Most Informative Features\n",
      "                dustbins = True              pos : neg    =      6.0 : 1.0\n",
      "                    next = True              pos : neg    =      3.6 : 1.0\n",
      "                   right = True              pos : neg    =      2.6 : 1.0\n",
      "                  thrown = True              pos : neg    =      2.0 : 1.0\n",
      "                  filled = False             neg : pos    =      2.0 : 1.0\n",
      "                 plastic = False             neg : pos    =      1.4 : 1.0\n",
      "                  dumped = False             pos : neg    =      1.3 : 1.0\n",
      "                  coming = False             neg : pos    =      1.2 : 1.0\n",
      "                    main = False             neg : pos    =      1.2 : 1.0\n",
      "                    cows = False             neg : pos    =      1.2 : 1.0\n",
      "                  eating = False             neg : pos    =      1.2 : 1.0\n",
      "                  parked = False             neg : pos    =      1.2 : 1.0\n",
      "                dustbins = False             neg : pos    =      1.2 : 1.0\n",
      "                    next = False             neg : pos    =      1.1 : 1.0\n",
      "                   right = False             neg : pos    =      1.1 : 1.0\n",
      "MNB_classifier accuracy percent: 95.23809523809523\n",
      "BernoulliNB_classifier accuracy percent: 80.95238095238095\n",
      "LogisticRegression_classifier accuracy percent: 90.47619047619048\n",
      "SGDClassifier_classifier accuracy percent: 95.23809523809523\n",
      "SVC_classifier accuracy percent: 95.23809523809523\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "MNB_clf = SklearnClassifier(MultinomialNB())\n",
    "MNB_clf.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_clf, testing_set))*100)\n",
    "\n",
    "BNB_clf = SklearnClassifier(BernoulliNB())\n",
    "BNB_clf.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BNB_clf, testing_set))*100)\n",
    "\n",
    "LogReg_clf = SklearnClassifier(LogisticRegression())\n",
    "LogReg_clf.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogReg_clf, testing_set))*100)\n",
    "\n",
    "SGD_clf = SklearnClassifier(SGDClassifier())\n",
    "SGD_clf.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGD_clf, testing_set))*100)\n",
    "\n",
    "SVC_clf = SklearnClassifier(SVC())\n",
    "SVC_clf.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_clf, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_pickle(c, file_name): \n",
    "    save_classifier = open(file_name, 'wb')\n",
    "    pickle.dump(c, save_classifier)\n",
    "    save_classifier.close()\n",
    "\n",
    "classifiers_dict = {'ONB': [classifier, 'pickled_algos/ONB_clf.pickle'],\n",
    "                    'MNB': [MNB_clf, 'pickled_algos/MNB_clf.pickle'],\n",
    "                    'BNB': [BNB_clf, 'pickled_algos/BNB_clf.pickle'],\n",
    "                    'LogReg': [LogReg_clf, 'pickled_algos/LogReg_clf.pickle'],\n",
    "                    'SGD': [SGD_clf, 'pickled_algos/SGD_clf.pickle'], \n",
    "                    'SVC': [SVC_clf, 'pickled_algos/SVC_clf.pickle']}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for clf, listy in classifiers_dict.items(): \n",
    "    create_pickle(listy[0], listy[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score ONB: 0.9047619047619048\n",
      "f1_score MNB: 0.9523809523809523\n",
      "f1_score BNB: 0.8095238095238095\n",
      "f1_score LogReg: 0.9047619047619048\n",
      "f1_score SGD: 0.9523809523809523\n",
      "f1_score SVC: 0.9523809523809523\n",
      "Accuracy_score ONB: 0.9047619047619048\n",
      "Accuracy_score MNB: 0.9523809523809523\n",
      "Accuracy_score BNB: 0.8095238095238095\n",
      "Accuracy_score LogReg: 0.9047619047619048\n",
      "Accuracy_score SGD: 0.9523809523809523\n",
      "Accuracy_score SVC: 0.9523809523809523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9047619047619048"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "'''acc_scores = {}\n",
    "for clf, listy in classifiers_dict.items(): \n",
    "    # getting predictions for the testing set by looping over each reviews featureset tuple\n",
    "    # The first elemnt of the tuple is the feature set and the second element is the label \n",
    "    acc_scores[clf] = accuracy_score(ground_truth, predictions[clf])\n",
    "    print(f'Accuracy_score {clf}: {acc_scores[clf]}') '''\n",
    "ground_truth = [r[1] for r in testing_set]\n",
    "predictions = {}\n",
    "f1_scores = {}\n",
    "for clf, listy in classifiers_dict.items(): \n",
    "    # getting predictions for the testing set by looping over each reviews featureset tuple\n",
    "    # The first elemnt of the tuple is the feature set and the second element is the label \n",
    "    predictions[clf] = [listy[0].classify(r[0]) for r in testing_set]\n",
    "    f1_scores[clf] = f1_score(ground_truth, predictions[clf], labels = ['neg', 'pos'], average = 'micro')\n",
    "    print(f'f1_score {clf}: {f1_scores[clf]}')\n",
    "\n",
    "acc_scores = {}\n",
    "for clf, listy in classifiers_dict.items(): \n",
    "    # getting predictions for the testing set by looping over each reviews featureset tuple\n",
    "    # The first elemnt of the tuple is the feature set and the second element is the label \n",
    "    acc_scores[clf] = accuracy_score(ground_truth, predictions[clf])\n",
    "    print(f'Accuracy_score {clf}: {acc_scores[clf]}')\n",
    "\n",
    "\n",
    "from nltk.classify import ClassifierI\n",
    "\n",
    "# Defininig the ensemble model class \n",
    "\n",
    "class EnsembleClassifier(ClassifierI):\n",
    "    \n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    # returns the classification based on majority of votes\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    # a simple measurement the degree of confidence in the classification \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "\n",
    "# Load all classifiers from the pickled files \n",
    "\n",
    "# function to load models given filepath\n",
    "def load_model(file_path): \n",
    "    classifier_f = open(file_path, \"rb\")\n",
    "    classifier = pickle.load(classifier_f)\n",
    "    classifier_f.close()\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# Original Naive Bayes Classifier\n",
    "ONB_Clf = load_model('pickled_algos/ONB_clf.pickle')\n",
    "\n",
    "# Multinomial Naive Bayes Classifier \n",
    "MNB_Clf = load_model('pickled_algos/MNB_clf.pickle')\n",
    "\n",
    "\n",
    "# Bernoulli  Naive Bayes Classifier \n",
    "BNB_Clf = load_model('pickled_algos/BNB_clf.pickle')\n",
    "\n",
    "# Logistic Regression Classifier \n",
    "LogReg_Clf = load_model('pickled_algos/LogReg_clf.pickle')\n",
    "\n",
    "# Stochastic Gradient Descent Classifier\n",
    "SGD_Clf = load_model('pickled_algos/SGD_clf.pickle')\n",
    "\n",
    "\n",
    "\n",
    "# Initializing the ensemble classifier \n",
    "ensemble_clf = EnsembleClassifier(ONB_Clf, MNB_Clf, BNB_Clf, LogReg_Clf, SGD_Clf)\n",
    "\n",
    "# List of only feature dictionary from the featureset list of tuples \n",
    "feature_list = [f[0] for f in testing_set]\n",
    "\n",
    "# Looping over each to classify each review\n",
    "ensemble_preds = [ensemble_clf.classify(features) for features in feature_list]\n",
    "\n",
    "f1_score(ground_truth, ensemble_preds, average = 'micro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to do classification a given review and return the label a\n",
    "# and the amount of confidence in the classifications\n",
    "def sentiment(text):\n",
    "    feats = find_features(text)\n",
    "    return ensemble_clf.classify(feats), ensemble_clf.confidence(feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('neg', 1.0), ('neg', 1.0), ('neg', 1.0), ('neg', 1.0), ('neg', 1.0))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment analysis of reviews of captain marvel found on rotten tomatoes\n",
    "text_a = '''It's in Besamt Nagar beach. All dustbins are broken and waste is thrown all around.'''\n",
    "text_b = '''Close to my home, there is a big bin for dumping garbage. But, people are throwing away garbage all around the bi'''\n",
    "text_c = '''It has a heap of garbage all wet and dry mixed together and there are cows digging through the this heap. Not only this but there is also so much of smell which causes lots of inconvenience for people'''\n",
    "text_d = '''Lot of garbage right outside the general hospital jayanagar. The garbage has been lying here for days and it stinks too. How can we care for the patients in such an environment?'''\n",
    "text_e = '''\"flowers and children are happy\"'''\n",
    "\n",
    "sentiment(text_a), sentiment(text_b), sentiment(text_c), sentiment(text_d), sentiment(text_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
